{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERVIEW**\n",
    "\n",
    "In this application, you will explore a dataset from Kaggle. The original dataset contained information on 3 million used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CRISP-DM Framework\n",
    "\n",
    "<center>\n",
    "    <img src = images/crisp.png width = 50%/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To frame the task, throughout our practical applications, we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Business Understanding\n",
    "\n",
    "From a business perspective, we are tasked with identifying key drivers for used car prices.  In the CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few sentences, reframe the task as a data task with the appropriate technical vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MY ANSWER:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a cross sectional data set (not a time series) and we will use supervised learning to find what variables (columns and combinations of columns) most closely correlate with the price.\n",
    "We will have success when we have identified which of the columns/variables are most important to create  a model that predicts the expected price that a car will sell for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Data Understanding\n",
    "\n",
    "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in the data to a Pandas Data Frame**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to read in the data file so we can examine its structure (rows and columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 426880 entries, 0 to 426879\n",
      "Data columns (total 18 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   id            426880 non-null  int64  \n",
      " 1   region        426880 non-null  object \n",
      " 2   price         426880 non-null  int64  \n",
      " 3   year          425675 non-null  float64\n",
      " 4   manufacturer  409234 non-null  object \n",
      " 5   model         421603 non-null  object \n",
      " 6   condition     252776 non-null  object \n",
      " 7   cylinders     249202 non-null  object \n",
      " 8   fuel          423867 non-null  object \n",
      " 9   odometer      422480 non-null  float64\n",
      " 10  title_status  418638 non-null  object \n",
      " 11  transmission  424324 non-null  object \n",
      " 12  VIN           265838 non-null  object \n",
      " 13  drive         296313 non-null  object \n",
      " 14  size          120519 non-null  object \n",
      " 15  type          334022 non-null  object \n",
      " 16  paint_color   296677 non-null  object \n",
      " 17  state         426880 non-null  object \n",
      "dtypes: float64(2), int64(2), object(14)\n",
      "memory usage: 58.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Global warning configuration\n",
    "# ================================\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=ConvergenceWarning\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# unclean_cars_data = original data before data preparation/cleaning\n",
    "unclean_cars_data = pd.read_csv(\"data/vehicles.csv\")\n",
    "\n",
    "# The data as it is being cleaned is 'cars' (shorter for simplicity):\n",
    "cars = []\n",
    "\n",
    "# Setting a global unchanging reference to keep track\n",
    "# of changes from the initial number of rows:\n",
    "INITIAL_ROW_COUNT = len(unclean_cars_data)\n",
    "\n",
    "print(unclean_cars_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                  region  price  year manufacturer model  \\\n",
      "0  7222695916                prescott   6000   NaN          NaN   NaN   \n",
      "1  7218891961            fayetteville  11900   NaN          NaN   NaN   \n",
      "2  7221797935            florida keys  21000   NaN          NaN   NaN   \n",
      "3  7222270760  worcester / central MA   1500   NaN          NaN   NaN   \n",
      "4  7210384030              greensboro   4900   NaN          NaN   NaN   \n",
      "\n",
      "  condition cylinders fuel  odometer title_status transmission  VIN drive  \\\n",
      "0       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "1       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "2       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "3       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "4       NaN       NaN  NaN       NaN          NaN          NaN  NaN   NaN   \n",
      "\n",
      "  size type paint_color state  \n",
      "0  NaN  NaN         NaN    az  \n",
      "1  NaN  NaN         NaN    ar  \n",
      "2  NaN  NaN         NaN    fl  \n",
      "3  NaN  NaN         NaN    ma  \n",
      "4  NaN  NaN         NaN    nc  \n"
     ]
    }
   ],
   "source": [
    "print(unclean_cars_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column: id\n",
      "[7222695916 7218891961 7221797935 ... 7301591147 7301591140 7301591129]\n",
      "\n",
      "Column: region\n",
      "['prescott' 'fayetteville' 'florida keys' 'worcester / central MA'\n",
      " 'greensboro' 'hudson valley' 'medford-ashland' 'erie' 'el paso'\n",
      " 'bellingham' 'skagit / island / SJI' 'la crosse' 'auburn' 'birmingham'\n",
      " 'dothan' 'florence / muscle shoals' 'gadsden-anniston'\n",
      " 'huntsville / decatur' 'mobile' 'montgomery' 'tuscaloosa'\n",
      " 'anchorage / mat-su' 'fairbanks' 'kenai peninsula' 'southeast alaska'\n",
      " 'flagstaff / sedona' 'mohave county' 'phoenix' 'show low' 'sierra vista'\n",
      " 'tucson' 'yuma' 'fort smith' 'jonesboro' 'little rock' 'texarkana'\n",
      " 'bakersfield' 'chico' 'fresno / madera' 'gold country' 'hanford-corcoran'\n",
      " 'humboldt county' 'imperial county' 'inland empire' 'los angeles'\n",
      " 'mendocino county' 'merced' 'modesto' 'monterey bay' 'orange county'\n",
      " 'palm springs' 'redding' 'reno / tahoe' 'sacramento' 'san diego'\n",
      " 'san luis obispo' 'santa barbara' 'santa maria' 'SF bay area'\n",
      " 'siskiyou county' 'stockton' 'susanville' 'ventura county'\n",
      " 'visalia-tulare' 'yuba-sutter' 'boulder' 'colorado springs' 'denver'\n",
      " 'eastern CO' 'fort collins / north CO' 'high rockies' 'pueblo'\n",
      " 'western slope' 'eastern CT' 'hartford' 'new haven' 'northwest CT'\n",
      " 'washington, DC' 'delaware' 'daytona beach' 'ft myers / SW florida'\n",
      " 'gainesville' 'heartland florida' 'jacksonville' 'lakeland'\n",
      " 'north central FL' 'ocala' 'okaloosa / walton' 'orlando' 'panama city'\n",
      " 'pensacola' 'sarasota-bradenton' 'south florida' 'space coast'\n",
      " 'st augustine' 'tallahassee' 'tampa bay area' 'treasure coast' 'albany'\n",
      " 'athens' 'atlanta' 'augusta' 'brunswick' 'columbus'\n",
      " 'macon / warner robins' 'northwest GA' 'savannah / hinesville'\n",
      " 'statesboro' 'valdosta' 'hawaii' 'boise' 'east idaho'\n",
      " 'lewiston / clarkston' 'pullman / moscow' \"spokane / coeur d'alene\"\n",
      " 'twin falls' 'bloomington-normal' 'champaign urbana' 'chicago' 'decatur'\n",
      " 'la salle co' 'mattoon-charleston' 'peoria' 'quad cities, IA/IL'\n",
      " 'rockford' 'southern illinois' 'springfield' 'st louis, MO' 'western IL'\n",
      " 'bloomington' 'evansville' 'fort wayne' 'indianapolis' 'kokomo'\n",
      " 'lafayette / west lafayette' 'muncie / anderson' 'richmond'\n",
      " 'south bend / michiana' 'terre haute' 'ames' 'cedar rapids' 'des moines'\n",
      " 'dubuque' 'fort dodge' 'iowa city' 'mason city' 'omaha / council bluffs'\n",
      " 'sioux city' 'southeast IA' 'waterloo / cedar falls' 'kansas city, MO'\n",
      " 'lawrence' 'manhattan' 'northwest KS' 'salina' 'southeast KS'\n",
      " 'southwest KS' 'topeka' 'wichita' 'bowling green' 'eastern kentucky'\n",
      " 'huntington-ashland' 'lexington' 'louisville' 'owensboro' 'western KY'\n",
      " 'baton rouge' 'central louisiana' 'houma' 'lafayette' 'lake charles'\n",
      " 'monroe' 'new orleans' 'shreveport' 'maine' 'annapolis' 'baltimore'\n",
      " 'cumberland valley' 'eastern shore' 'frederick' 'southern maryland'\n",
      " 'western maryland' 'boston' 'cape cod / islands' 'south coast'\n",
      " 'western massachusetts' 'ann arbor' 'battle creek' 'central michigan'\n",
      " 'detroit metro' 'flint' 'grand rapids' 'holland' 'jackson' 'kalamazoo'\n",
      " 'lansing' 'muskegon' 'northern michigan' 'port huron'\n",
      " 'saginaw-midland-baycity' 'southwest michigan' 'the thumb'\n",
      " 'upper peninsula' 'bemidji' 'brainerd' 'duluth / superior'\n",
      " 'fargo / moorhead' 'mankato' 'minneapolis / st paul' 'rochester'\n",
      " 'southwest MN' 'st cloud' 'gulfport / biloxi' 'hattiesburg' 'meridian'\n",
      " 'north mississippi' 'southwest MS' 'columbia / jeff city' 'joplin'\n",
      " 'kansas city' 'kirksville' 'lake of the ozarks' 'southeast missouri'\n",
      " 'st joseph' 'st louis' 'billings' 'bozeman' 'butte' 'eastern montana'\n",
      " 'great falls' 'helena' 'kalispell' 'missoula' 'asheville' 'boone'\n",
      " 'charlotte' 'eastern NC' 'hickory / lenoir' 'outer banks'\n",
      " 'raleigh / durham / CH' 'wilmington' 'winston-salem' 'grand island'\n",
      " 'lincoln' 'north platte' 'scottsbluff / panhandle' 'elko' 'las vegas'\n",
      " 'central NJ' 'jersey shore' 'north jersey' 'south jersey' 'albuquerque'\n",
      " 'clovis / portales' 'farmington' 'las cruces' 'roswell / carlsbad'\n",
      " 'santa fe / taos' 'binghamton' 'buffalo' 'catskills' 'chautauqua'\n",
      " 'elmira-corning' 'finger lakes' 'glens falls' 'ithaca' 'long island'\n",
      " 'new york city' 'oneonta' 'plattsburgh-adirondacks'\n",
      " 'potsdam-canton-massena' 'syracuse' 'twin tiers NY/PA'\n",
      " 'utica-rome-oneida' 'watertown' 'new hampshire' 'bismarck' 'grand forks'\n",
      " 'north dakota' 'akron / canton' 'ashtabula' 'chillicothe' 'cincinnati'\n",
      " 'cleveland' 'dayton / springfield' 'lima / findlay' 'mansfield'\n",
      " 'northern panhandle' 'parkersburg-marietta' 'sandusky' 'toledo'\n",
      " 'tuscarawas co' 'youngstown' 'zanesville / cambridge' 'fort smith, AR'\n",
      " 'lawton' 'northwest OK' 'oklahoma city' 'stillwater' 'texoma' 'tulsa'\n",
      " 'bend' 'corvallis/albany' 'east oregon' 'eugene' 'klamath falls'\n",
      " 'oregon coast' 'portland' 'roseburg' 'salem' 'altoona-johnstown'\n",
      " 'harrisburg' 'lancaster' 'lehigh valley' 'meadville' 'philadelphia'\n",
      " 'pittsburgh' 'poconos' 'reading' 'scranton / wilkes-barre'\n",
      " 'state college' 'williamsport' 'york' 'rhode island' 'charleston'\n",
      " 'columbia' 'florence' 'greenville / upstate' 'hilton head' 'myrtle beach'\n",
      " 'northeast SD' 'pierre / central SD' 'rapid city / west SD'\n",
      " 'sioux falls / SE SD' 'south dakota' 'chattanooga' 'clarksville'\n",
      " 'cookeville' 'knoxville' 'memphis' 'nashville' 'tri-cities' 'abilene'\n",
      " 'amarillo' 'austin' 'beaumont / port arthur' 'brownsville'\n",
      " 'college station' 'corpus christi' 'dallas / fort worth'\n",
      " 'deep east texas' 'del rio / eagle pass' 'galveston' 'houston'\n",
      " 'killeen / temple / ft hood' 'laredo' 'lubbock' 'mcallen / edinburg'\n",
      " 'odessa / midland' 'san angelo' 'san antonio' 'san marcos' 'southwest TX'\n",
      " 'tyler / east TX' 'victoria' 'waco' 'wichita falls' 'logan'\n",
      " 'ogden-clearfield' 'provo / orem' 'salt lake city' 'st george' 'vermont'\n",
      " 'charlottesville' 'danville' 'fredericksburg' 'harrisonburg' 'lynchburg'\n",
      " 'new river valley' 'norfolk / hampton roads' 'roanoke' 'southwest VA'\n",
      " 'winchester' 'kennewick-pasco-richland' 'moses lake' 'olympic peninsula'\n",
      " 'seattle-tacoma' 'wenatchee' 'yakima' 'eastern panhandle' 'morgantown'\n",
      " 'southern WV' 'west virginia (old)' 'appleton-oshkosh-FDL' 'eau claire'\n",
      " 'green bay' 'janesville' 'kenosha-racine' 'madison' 'milwaukee'\n",
      " 'northern WI' 'sheboygan' 'wausau' 'wyoming']\n",
      "\n",
      "Column: price\n",
      "[ 6000 11900 21000 ...  6328 19853 17873]\n",
      "\n",
      "Column: year\n",
      "[  nan 2014. 2010. 2020. 2017. 2013. 2012. 2016. 2019. 2011. 1992. 2018.\n",
      " 2004. 2015. 2001. 2006. 1968. 2003. 2008. 2007. 2005. 1966. 2009. 1998.\n",
      " 2002. 1999. 2021. 1997. 1976. 1969. 1995. 1978. 1954. 1979. 1970. 1974.\n",
      " 1996. 1987. 2000. 1955. 1960. 1991. 1972. 1988. 1994. 1929. 1984. 1986.\n",
      " 1989. 1973. 1946. 1933. 1958. 1937. 1985. 1957. 1953. 1942. 1963. 1977.\n",
      " 1993. 1903. 1990. 1965. 1982. 1948. 1983. 1936. 1932. 1951. 1931. 1980.\n",
      " 1967. 1971. 1947. 1981. 1926. 1962. 1975. 1964. 1934. 1952. 1940. 1959.\n",
      " 1950. 1930. 1956. 1922. 1928. 2022. 1901. 1941. 1924. 1927. 1939. 1923.\n",
      " 1949. 1961. 1935. 1918. 1900. 1938. 1913. 1916. 1943. 1925. 1921. 1915.\n",
      " 1945. 1902. 1905. 1920. 1944. 1910. 1909.]\n",
      "\n",
      "Column: manufacturer\n",
      "[nan 'gmc' 'chevrolet' 'toyota' 'ford' 'jeep' 'nissan' 'ram' 'mazda'\n",
      " 'cadillac' 'honda' 'dodge' 'lexus' 'jaguar' 'buick' 'chrysler' 'volvo'\n",
      " 'audi' 'infiniti' 'lincoln' 'alfa-romeo' 'subaru' 'acura' 'hyundai'\n",
      " 'mercedes-benz' 'bmw' 'mitsubishi' 'volkswagen' 'porsche' 'kia' 'rover'\n",
      " 'ferrari' 'mini' 'pontiac' 'fiat' 'tesla' 'saturn' 'mercury'\n",
      " 'harley-davidson' 'datsun' 'aston-martin' 'land rover' 'morgan']\n",
      "\n",
      "Column: model\n",
      "[nan 'sierra 1500 crew cab slt' 'silverado 1500' ... 'gand wagoneer'\n",
      " '96 Suburban' 'Paige Glenbrook Touring']\n",
      "\n",
      "Column: condition\n",
      "[nan 'good' 'excellent' 'fair' 'like new' 'new' 'salvage']\n",
      "\n",
      "Column: cylinders\n",
      "[nan '8 cylinders' '6 cylinders' '4 cylinders' '5 cylinders' 'other'\n",
      " '3 cylinders' '10 cylinders' '12 cylinders']\n",
      "\n",
      "Column: fuel\n",
      "[nan 'gas' 'other' 'diesel' 'hybrid' 'electric']\n",
      "\n",
      "Column: odometer\n",
      "[    nan  57923.  71229. ... 172511.  94964.  26892.]\n",
      "\n",
      "Column: title_status\n",
      "[nan 'clean' 'rebuilt' 'lien' 'salvage' 'missing' 'parts only']\n",
      "\n",
      "Column: transmission\n",
      "[nan 'other' 'automatic' 'manual']\n",
      "\n",
      "Column: VIN\n",
      "[nan '3GTP1VEC4EG551563' '1GCSCSE06AZ123805' ... '2HGES15535H620534'\n",
      " '1FDWF37P64EA24868' 'SAJGX2749VCOO8376']\n",
      "\n",
      "Column: drive\n",
      "[nan 'rwd' '4wd' 'fwd']\n",
      "\n",
      "Column: size\n",
      "[nan 'full-size' 'mid-size' 'compact' 'sub-compact']\n",
      "\n",
      "Column: type\n",
      "[nan 'pickup' 'truck' 'other' 'coupe' 'SUV' 'hatchback' 'mini-van' 'sedan'\n",
      " 'offroad' 'bus' 'van' 'convertible' 'wagon']\n",
      "\n",
      "Column: paint_color\n",
      "[nan 'white' 'blue' 'red' 'black' 'silver' 'grey' 'brown' 'yellow'\n",
      " 'orange' 'green' 'custom' 'purple']\n",
      "\n",
      "Column: state\n",
      "['az' 'ar' 'fl' 'ma' 'nc' 'ny' 'or' 'pa' 'tx' 'wa' 'wi' 'al' 'ak' 'ca'\n",
      " 'co' 'ct' 'dc' 'de' 'ga' 'hi' 'id' 'il' 'in' 'ia' 'ks' 'ky' 'la' 'me'\n",
      " 'md' 'mi' 'mn' 'ms' 'mo' 'mt' 'ne' 'nv' 'nj' 'nm' 'nh' 'nd' 'oh' 'ok'\n",
      " 'ri' 'sc' 'sd' 'tn' 'ut' 'vt' 'va' 'wv' 'wy']\n"
     ]
    }
   ],
   "source": [
    "for col in unclean_cars_data.columns:\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(unclean_cars_data[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My observations of the data:\n",
    "\n",
    "\n",
    "1. Though the unique values output is lengthy, it is worth the relatively small investment in time to scroll down it. It showed what all the different kinds of values are and how few typos and nonsense data there is overall. Most columns contain useful values (apart from NaNs). For example 'type' has no entries like: \"speedy\" or \"the best ever.\" \n",
    "2. We can see that there is an extremely large number of different 'regions' - too many to be useful, especially when we also have the 'state' column which will have a maximum of about 50.\n",
    "3. We can see many empty data cells (NaN) so we have to further examine and decide what to do with rows that contain NaN values.\n",
    "4. It was also observed that a number of rows only have the following values: region, price and state. This is shown to be quite common in the 'head' 5 rows. It would seem necessary to remove these rows but in doing so it will be important to see just how many rows this removes (percentage of total rows) and whether the data remains sufficiently large to be used effectively.\n",
    "5. We can see some columns such as the VIN number for each individual car that will be of little use except to determine if there are repeated rows (duplicates) for the very same car.\n",
    "6. Overall we can see mostly non-numerical values (categorical values) that will need to be converted to a more readily usable form via One-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Data Preparation\n",
    "\n",
    "After our initial exploration and fine-tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up a row counting function to keep track of row removal:\n",
    "\n",
    "How much data are we removing? This function will be called to report this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def report_change(rows_before, df_after, step_description=\"this step\"):\n",
    "    \"\"\"\n",
    "    Report row-count changes for a data-cleaning step.\n",
    "\n",
    "    This function reports how many rows were removed during a single\n",
    "    data-cleaning operation and tracks cumulative removals relative to\n",
    "    the original dataset size.\n",
    "\n",
    "    Designed for large datasets: only row counts are used, and the\n",
    "    DataFrame is passed by reference (no copying).\n",
    "\n",
    "    Assumes `INITIAL_ROW_COUNT` is defined once at data-load time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rows_before : int\n",
    "        Number of rows before the cleaning step\n",
    "    df_after : pd.DataFrame\n",
    "        DataFrame after the cleaning step\n",
    "    step_description : str, optional\n",
    "        Short description of the cleaning step\n",
    "    \"\"\"\n",
    "    global INITIAL_ROW_COUNT\n",
    "\n",
    "    rows_after = len(df_after)\n",
    "\n",
    "    removed = rows_before - rows_after\n",
    "    total_removed = INITIAL_ROW_COUNT - rows_after\n",
    "\n",
    "    percent_this = (removed / INITIAL_ROW_COUNT) * 100 if rows_before > 0 else 0\n",
    "    percent_total = (total_removed / INITIAL_ROW_COUNT) * 100\n",
    "\n",
    "    print(f\"Row count before: {rows_before}\")\n",
    "    print(f\"Row count after: {rows_after}\")\n",
    "    print(f\"Rows removed this step: {removed}\")\n",
    "    print(f\"Rows removed were {step_description}.\")\n",
    "    print(f\"Percentage removed in this step: {percent_this:.2f}%\")\n",
    "    print(f\"Percentage removed so far: {percent_total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove mostly empty rows\n",
    "\n",
    "We want to know if cars (i.e sensible and unique VIN numbers) appear more than once with **ALL** the same information (apart from ID) and then remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count before: 426880\n",
      "Row count after: 426812\n",
      "Rows removed this step: 68\n",
      "Rows removed were missing all data (except the price, region, state).\n",
      "Percentage removed in this step: 0.02%\n",
      "Percentage removed so far: 0.02%\n"
     ]
    }
   ],
   "source": [
    "len_before=len(unclean_cars_data)\n",
    "\n",
    "cars = unclean_cars_data.dropna(subset=unclean_cars_data.columns.difference([\"id\", \"price\", \"region\", \"state\"]), how=\"all\")\n",
    "\n",
    "report_change(len_before,cars,\"missing all data (except the price, region, state)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove fully duplicated rows - only differing by 'id'\n",
    "\n",
    "We want to know if cars (i.e sensible and unique VIN numbers) appear more than once with **ALL** the same information (apart from ID) and then remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count before: 426812\n",
      "Row count after: 370621\n",
      "Rows removed this step: 56191\n",
      "Rows removed were fully duplicated rows (only id differed).\n",
      "Percentage removed in this step: 13.16%\n",
      "Percentage removed so far: 13.18%\n"
     ]
    }
   ],
   "source": [
    "len_before=len(cars)\n",
    "\n",
    "cars = cars.sort_values(\"id\").drop_duplicates(subset=cars.columns.difference([\"id\"]), keep=\"first\")\n",
    "\n",
    "report_change(len_before,cars,\"fully duplicated rows (only id differed)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dealing with mulpitle entries with the same VIN:\n",
    "\n",
    "Now we want to see what columns might differ for rows that still have the same VIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with the same VIN but differing only in these columns:\n",
      "VIN\n",
      "0                    [id, region, price, year, manufacturer, model,...\n",
      "00000000000A26444                                                   []\n",
      "00000000000A35665                                                   []\n",
      "00000000000A42482                                                   []\n",
      "00000000000A44554                                                   []\n",
      "                                           ...                        \n",
      "ZN661YUA4LX344692                                         [id, region]\n",
      "ZN661YUL1HX236166                                                   []\n",
      "ZN661YUL3HX198178                                                   []\n",
      "ZN661YUS0HX263351                                                   []\n",
      "ZPBUA1ZL1KLA02237                                  [id, region, state]\n",
      "Length: 118246, dtype: object\n"
     ]
    }
   ],
   "source": [
    "cars_multi_VINs = (\n",
    "    cars\n",
    "    .groupby(\"VIN\")\n",
    "    .nunique()\n",
    "    .apply(lambda row: row[row > 1].index.tolist(), axis=1)\n",
    ")\n",
    "\n",
    "print(\"Columns with the same VIN but differing only in these columns:\")\n",
    "print(cars_multi_VINs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations** It can be seen that:\n",
    "\n",
    "(1) There are many entires that differ only in the location (id, region, state) and so are probably being listed in multiple places at once to increase exposure and the chance of an interested party.  This means that they are 'polluting' the location information (region & state) and making it less helpful for correlating with the price. We can't know where the car was actually located in real world for those mutiples. Also the region contains such a variety of locations, it would be hard to make use of it as it is. Even using the 'state' risks more time consumption to investigate correlation which may just result in false outcomes because many cars are not physically located in the 'state' that they are listed under (also the multiple entires for one car).\n",
    "\n",
    "(2) Also from general knowledge, region also has competing factors such as proximity to the ocean can imply more chance of rust issues but population density is often higher along the coast so this may affect the price.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) A VIN (00000HDJ810018347) has many multiple entries ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             region   year                          model               VIN state\n",
      "dallas / fort worth 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    tx\n",
      "             denver 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    co\n",
      "        east oregon 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    or\n",
      "      new york city 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    ny\n",
      "      oklahoma city 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    ok\n",
      "          asheville 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    nc\n",
      "            atlanta 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    ga\n",
      "              boise 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    id\n",
      "             boston 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    ma\n",
      "            buffalo 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    ny\n",
      "         charleston 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    sc\n",
      "            chicago 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    il\n",
      "          cleveland 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    oh\n",
      "           columbus 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    oh\n",
      "            el paso 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    tx\n",
      "             eugene 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    or\n",
      "       harrisonburg 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    va\n",
      "            houston 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    tx\n",
      "       jacksonville 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    fl\n",
      "    kansas city, MO 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    ks\n",
      "          las vegas 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    nv\n",
      "        long island 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    ny\n",
      "        los angeles 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    ca\n",
      "              maine 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    me\n",
      "          nashville 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    tn\n",
      "       palm springs 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    ca\n",
      "            phoenix 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    az\n",
      "           portland 1992.0 land cruiser hdj81 - 80 series 00000HDJ810018347    or\n"
     ]
    }
   ],
   "source": [
    "cols = [\"region\", \"year\", \"model\", \"VIN\", \"state\"]\n",
    "print(cars.loc[cars[\"VIN\"] == \"00000HDJ810018347\", cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each car listed like this polutes the location columns (region, state) drastically.\n",
    "\n",
    "**TWO OPTIONS:** Either remove the entire 'region' and the entire 'state' columns (and then remove duplicate rows) **OR** remove the cars/rows where they are listed in multiple locations (it is only one car afterall) so that the location (especially 'state') can be used for the other cars.\n",
    "\n",
    "**DECISION:**\n",
    "\n",
    "- Remove the rows with the same details except where the 'region' is the only difference so we can still use the 'state' column.\n",
    "- Remove the 'region' column since some sense of location is capture by the 'state' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count before: 370621\n",
      "Row count after: 248689\n",
      "Rows removed this step: 121932\n",
      "Rows removed were mostly duplicated rows (only location differed).\n",
      "Percentage removed in this step: 28.56%\n",
      "Percentage removed so far: 41.74%\n"
     ]
    }
   ],
   "source": [
    "# First, remove the duplicated rows that differ only by location: \n",
    "len_before=len(cars)\n",
    "\n",
    "cars = cars.drop_duplicates(\n",
    "    subset=cars.columns.difference([\"id\", \"region\", \"state\"]),\n",
    "    keep=\"first\"\n",
    ")\n",
    "report_change(len_before,cars,\"mostly duplicated rows (only location differed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second remove the region column/feature (so no row/data changes):\n",
    "cars = cars.drop(columns=[\"region\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Same car but different price? So keep lowest price row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " price   year  manufacturer  model  odometer         VIN state\n",
      " 22000 1979.0 mercedes-benz sl 450   70000.0 1.07044E+11    ca\n",
      " 28999 1979.0 mercedes-benz sl 450   70000.0 1.07044E+11    ca\n",
      " 27900 1979.0 mercedes-benz sl 450   70000.0 1.07044E+11    ca\n"
     ]
    }
   ],
   "source": [
    "# An example for VIN = 1.07044E+11 with multiple entries ...\n",
    "\n",
    "cols = [\"price\", \"year\", \"manufacturer\", \"model\", \"odometer\", \"VIN\", \"state\"]\n",
    "\n",
    "print(\n",
    "    cars.loc[cars[\"VIN\"] == \"1.07044E+11\", cols]\n",
    "        .sort_values(\"year\")\n",
    "        .to_string(index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the issue with using rows like this (and removing a duplicate) is 'What price should we use?' The price presumably went down when it was not purchased at the higher price, otherwise it would not still be listed. It would make no sense to increase the price if the car was not sold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DECISION:** Remove duplicates but keep the row with the lowest price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count before: 248689\n",
      "Row count after: 244479\n",
      "Rows removed this step: 4210\n",
      "Rows removed were duplicated in all except price (lowest kept).\n",
      "Percentage removed in this step: 0.99%\n",
      "Percentage removed so far: 42.73%\n"
     ]
    }
   ],
   "source": [
    "len_before=len(cars)\n",
    "\n",
    "# Define all columns except id and price\n",
    "cols_except_id_price = cars.columns.difference([\"id\", \"price\"])\n",
    "\n",
    "# Sort by price so lowest price comes first, if there are duplicates\n",
    "cars_lowest_price = (\n",
    "    cars.sort_values(\"price\")\n",
    "        .drop_duplicates(subset=cols_except_id_price, keep=\"first\")\n",
    ")\n",
    "\n",
    "# Set the cars data frame to be the new reduced data frame:\n",
    "cars = cars_lowest_price\n",
    "\n",
    "report_change(len_before,cars,\"duplicated in all except price (lowest kept)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Rows where the manufacturer is NaN?\n",
    "Should we leave out rows where there are blank entries for 'manufacturer' since that the manufacturer is pretty core data?\n",
    "\n",
    "(1) Since the lack of manufacturer loses a strong predictor (i.e. inserting a dummy like 'unknown' will pollute the data). So if there is only a small number of rows (say less than 5%) then we can leave the rows out for a better model without the negatives being too strong.\n",
    "\n",
    "(2) If the number of rows with no manufacturer is greater than 20%, then removing the rows would remove too much other data and we might need to insert dummy \"unknown\" instead.\n",
    "\n",
    "(3) This would leave rows out where the manufacturer is written in the model (by mistake) or can be determined by the model. There is not sufficient time to address these issue of information in the wrong column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPOTHETICALLY ... how much would go if rows with no manufacturer were removed?\n",
      "Row count before: 244479\n",
      "Row count after: 233776\n",
      "Rows removed this step: 10703\n",
      "Rows removed were only hypothetically not actually removed.\n",
      "Percentage removed in this step: 2.51%\n",
      "Percentage removed so far: 45.24%\n"
     ]
    }
   ],
   "source": [
    "len_before=len(cars)\n",
    "\n",
    "cars_no_manufacturer_gone = cars[cars[\"manufacturer\"].notna() & (cars[\"manufacturer\"].str.strip() != \"\")]\n",
    "\n",
    "print(\"HYPOTHETICALLY ... how much would go if rows with no manufacturer were removed?\")\n",
    "report_change(len_before,cars_no_manufacturer_gone,\"only hypothetically not actually removed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBSERVATIONS/ACTION for B:** Remove rows with manufacturer as NaN because it is less than 5% of rows that have no manufacturer, this will not significantly lose too much data and will allow for a better model of other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count before: 244479\n",
      "Row count after: 233776\n",
      "Rows removed this step: 10703\n",
      "Rows removed were without manufacturer.\n",
      "Percentage removed in this step: 2.51%\n",
      "Percentage removed so far: 45.24%\n"
     ]
    }
   ],
   "source": [
    "# update cars to the hypothetically reduced df:\n",
    "cars = cars_no_manufacturer_gone\n",
    "report_change(len_before,cars,\"without manufacturer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Other essential Fields: 'price' and 'odometer'\n",
    "\n",
    "'price' is the necessary target so not having it is significant\n",
    "\n",
    "'odometer' like year\n",
    "\n",
    "**Action:** Remove rows with no price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count before: 233776\n",
      "Row count after: 231283\n",
      "Rows removed this step: 2493\n",
      "Rows removed were without price or odometer figure.\n",
      "Percentage removed in this step: 0.58%\n",
      "Percentage removed so far: 45.82%\n"
     ]
    }
   ],
   "source": [
    "len_before=len(cars)\n",
    "\n",
    "# Remove the rows with no price target\n",
    "cars = cars.dropna(subset=[\"odometer\", \"price\"])\n",
    "\n",
    "report_change(len_before,cars,\"without price or odometer figure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Hypothetically - What % would be lost if all rows with blank entries were removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_NaN_for_blanks = cars.replace(r'^\\s*$', pd.NA, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_with_all_fields = cars_NaN_for_blanks.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPOTHETICALLY ... how much would go now if rows any NaNs were removed?\n",
      "Row count before: 231283\n",
      "Row count after: 22582\n",
      "Rows removed this step: 208701\n",
      "Rows removed were hypothetically removed for having any NaNs.\n",
      "Percentage removed in this step: 48.89%\n",
      "Percentage removed so far: 94.71%\n"
     ]
    }
   ],
   "source": [
    "len_before=len(cars)\n",
    "\n",
    "cars_NaN_for_blanks = cars.replace(r'^\\s*$', pd.NA, regex=True)\n",
    "cars_with_all_fields = cars_NaN_for_blanks.dropna()\n",
    "\n",
    "# Number of rows removed if we kept only rows with all fields:\n",
    "print(\"HYPOTHETICALLY ... how much would go now if rows any NaNs were removed?\")\n",
    "report_change(len_before,cars_with_all_fields,\"hypothetically removed for having any NaNs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBSERVATIONS:**\n",
    "\n",
    "(1) Clearly it would remove too many rows to only keep rows that have data in all remaining columns.\n",
    "\n",
    "(2) Just leave the missing data (NaN) to be dealt with in the preprocessing pipeline by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "    SimpleImputer(strategy=\"most_frequent\"),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. The quality of the remaining manufactuer column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIQUE MANUFACTURERS:\n",
      "acura, alfa-romeo, aston-martin, audi, bmw, buick, cadillac, chevrolet, chrysler, datsun, dodge, ferrari, fiat, ford, gmc, harley-davidson, honda, hyundai, infiniti, jaguar, jeep, kia, land rover, lexus, lincoln, mazda, mercedes-benz, mercury, mini, mitsubishi, morgan, nissan, pontiac, porsche, ram, rover, saturn, subaru, tesla, toyota, volkswagen, volvo\n"
     ]
    }
   ],
   "source": [
    "print(\"UNIQUE MANUFACTURERS:\")\n",
    "print(\", \".join(sorted(cars[\"manufacturer\"].dropna().unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These manufacturers were fed into ChatGPT to see if they are all valid names.\n",
    "They are, though some are historical names that are no longer manufacturing: eg. Datsun is now Nissan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. The quality of the remaining 'price' column: absurd values or no values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting the file cars_eighth_step.csv by price revealed these rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price column has NaN: False\n",
      "  \n",
      "     price   year  manufacturer               VIN state\n",
      "    777777 2012.0           gmc               NaN    ok\n",
      "    990000 2017.0 mercedes-benz WDCYC7DF5HX267228    fl\n",
      "    999999 2006.0          ford               NaN    wi\n",
      "   1000000 2016.0          ford               NaN    nc\n",
      "   1111111 1999.0     chevrolet               NaN    mn\n",
      "   1111111 2014.0     chevrolet               NaN    ca\n",
      "   1111111 1994.0     chevrolet               NaN    nh\n",
      "   1111111 1970.0         dodge               NaN    ms\n",
      "   1111111 1966.0     chevrolet               NaN    tn\n",
      "   1234567 2006.0          jeep               NaN    in\n",
      "   1234567 2010.0       lincoln 2LMHJ5AT1ABJ50124    mi\n",
      "   1234567 1955.0     chevrolet               NaN    tx\n",
      "   1234567 1955.0     chevrolet               NaN    tx\n",
      "   2000000 2002.0        saturn               NaN    ca\n",
      "   6995495 2014.0         dodge 3C4PDCAB5ET204162    fl\n",
      "  10004000 2002.0          ford               NaN    oh\n",
      "  11111111 1966.0     chevrolet               NaN    tn\n",
      "  12345678 2019.0     chevrolet                 0    oh\n",
      "  17000000 2007.0           ram               NaN    id\n",
      "  25003000 1991.0     chevrolet               NaN    tx\n",
      "  99999999 1993.0          ford               NaN    al\n",
      " 113456789 1980.0           bmw               NaN    nc\n",
      " 123456789 1996.0           gmc               NaN    ok\n",
      " 123456789 2015.0     chevrolet 1G1PC5SB0F7246637    mi\n",
      " 123456789 1965.0     chevrolet               NaN    oh\n",
      " 135008900 2008.0        nissan               NaN    nc\n",
      " 987654321 1960.0     chevrolet               NaN    ga\n",
      " 987654321 1960.0     chevrolet               NaN    al\n",
      "1111111111 1999.0          ford               NaN    ca\n",
      "1111111111 2020.0          jeep               NaN    ca\n",
      "1234567890 2006.0         volvo               NaN    in\n",
      "1410065407 1989.0          jeep               NaN    md\n",
      "3009548743 2021.0     chevrolet               NaN    ca\n",
      "3024942282 2000.0 mercedes-benz               NaN    de\n",
      "3024942282 2000.0 mercedes-benz               NaN    nj\n",
      "3736928711 2007.0        toyota               NaN    or\n",
      "3736928711 1999.0        toyota               NaN    tn\n"
     ]
    }
   ],
   "source": [
    "# Check if 'price' column has any NaN values (type int64):\n",
    "has_missing = cars['price'].isna().any()\n",
    "print(\"Price column has NaN:\", has_missing)\n",
    "print(\"  \")\n",
    "\n",
    "columns_to_see = [\"price\",\"year\", \"manufacturer\", \"VIN\", \"state\"]\n",
    "\n",
    "print(\n",
    "    cars.loc[cars[\"price\"] > 655000, columns_to_see]\n",
    "        .sort_values(\"price\")\n",
    "        .to_string(index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, none of these rows are usable because the value does not match the rest of the details and they are excessively high without any unique vintage special cars that do not have clearly silly prices like \"1234567\". So we will remove these rows also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count before: 231283\n",
      "Row count after: 231246\n",
      "Rows removed this step: 37\n",
      "Rows removed were ones with clearly incorrect prices.\n",
      "Percentage removed in this step: 0.01%\n",
      "Percentage removed so far: 45.83%\n"
     ]
    }
   ],
   "source": [
    "len_before=len(cars)\n",
    "\n",
    "cars = cars[cars[\"price\"] <= 655000]\n",
    "\n",
    "# Number of rows removed if we kept only rows with all fields:\n",
    "report_change(len_before,cars,\"ones with clearly incorrect prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Any empty strings that might affect the one-hot encoding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty strings that might affect One-hot encoding? False\n"
     ]
    }
   ],
   "source": [
    "print( \"Empty strings that might affect One-hot encoding?\", (cars == \"\").any().any() )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11: Convert 'year' and 'odometer' to an integer (from float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = cars.astype({\n",
    "    \"year\": \"Int64\",\n",
    "    \"odometer\": \"Int64\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Best method for 'cylinders' column in our model: numerical or categorical\n",
    "\n",
    "I settled upon an effective way to get the most out of the cylinders column.\n",
    "\n",
    "**First**, I saw lots of numbers in the cylinder feature so I thought, \"why don't I convert that to just an integer so it can be fed in numerically, instead of categorically. The reasoning was that One-hot encoding multipled the number of columns and so I converted cylinders to a number with 0 as he value if there was no value.\n",
    "That is what I did in the first model. As per the rest of this workbook.\n",
    "\n",
    "**Second**, Because polynomial degree 5 was discovered to be best - see modelling and conclusion. This was done by trying polynomial values of 1 to 7. So having 3 numerical features/columns for the polynomial of degree 7 increases the processing time because it generates 285 features (3+9+19+34 ...+164+219=285). However, with only 2 initial numerical features, the code only creates 66 total features (2+3+ ..10+11=55).\n",
    "\n",
    "Then I reflected more on cylinder and did some research and since there are odd values like 5 cylinder which is not really something between 4 cylinders and 6 cylinders in terms of price. So my research pointed to it working better as a categorical feature which would reduce the polynomial expansion by only having two remaining numerical features ('year' and 'odometer'). So I removed this code below and adjusted other parts to compensate for 'cylinders' then being categorical.\n",
    "\n",
    "**DISCOVERY:** I then discovered the model performance actually dropped and the model was more unstable. Ideally this should be in the write up but there is no simple way to change the code to have it run with cylinders as catgories and also as numerical so I had to choose the best option for this workbook - there would be insufficient time to have both in this simple project (it is not a Capstone Project). In an ideal project, I would incorporate the results so it could be seen that it does in fact work more poorly as a category. However, the size and weight of this assinment does not allow me time to add that it. I researched and found that polynomial degree 5 was able to use the numerical cylinders to great effect because that degree was able to effectively map the oddities of the cylinder numbers and commine it numerically with the other numerical features. See below in the 'Evaluation' section.\n",
    "\n",
    "**Third**, so I re-inserted this code below (and the other changes) to made cylinders a numerical category again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4 cylinders' nan 'other' '6 cylinders' '8 cylinders' '10 cylinders'\n",
      " '5 cylinders' '3 cylinders' '12 cylinders']\n"
     ]
    }
   ],
   "source": [
    "print(cars[\"cylinders\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code below that converts cylinders to an int64.\n",
    "\n",
    "cars = cars.copy()  # If Pandas fears I'm modifying a slice of a parent DataFrame\n",
    "\n",
    "cars.loc[:, \"cylinders\"] = (\n",
    "    cars[\"cylinders\"]\n",
    "        .astype(\"string\")            # ensure string dtype\n",
    "         .str.extract(r\"(\\d+)\")[0]    # <-- pick the first column from extract\n",
    "         .fillna(\"0\")                 # 'other', blank, NaN  \"0\"\n",
    "         .astype(\"Int64\")             # nullable integer\n",
    " )\n",
    "# Ensure 'cylinders' datatype is Int64 as still said 'object':\n",
    "cars[\"cylinders\"] = cars[\"cylinders\"].astype(\"Int64\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Remove columns that won't be used: 'id' and 'VIN'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove columns id and VIN:\n",
    "cars = cars.drop(columns=[\"id\", \"VIN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Other decisions on notable features of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Leave out rows with 'price' as 0 or 1\n",
    "Some rows contain prices of 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count before: 231246\n",
      "Row count after: 214727\n",
      "Rows removed this step: 16519\n",
      "Rows removed were ones with price as $0 or $1.\n",
      "Percentage removed in this step: 3.87%\n",
      "Percentage removed so far: 49.70%\n"
     ]
    }
   ],
   "source": [
    "len_before=len(cars)\n",
    "\n",
    "cars = cars[cars[\"price\"] > 1]\n",
    "\n",
    "report_change(len_before,cars,\"ones with price as $0 or $1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Leave out the rows with the important 'odometer' factor wrongly set as 0 or 1 so the model is not distorted.\n",
    "Even brand new cars rarely have 0 or 1 mile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count before: 214727\n",
      "Row count after: 213185\n",
      "Rows removed this step: 1542\n",
      "Rows removed were ones with odometer as 0 or 1.\n",
      "Percentage removed in this step: 0.36%\n",
      "Percentage removed so far: 50.06%\n"
     ]
    }
   ],
   "source": [
    "len_before=len(cars)\n",
    "\n",
    "cars = cars[~cars[\"odometer\"].isin([0, 1])]\n",
    "\n",
    "report_change(len_before,cars,\"ones with odometer as 0 or 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii. There are many with 'model' as 'other' or 'intern' or just 'i'.\n",
    "**Action:** See 'iv' below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iv. Leave out the 'model' column:\n",
    "The model column is very messy with imprecise strings where the manufacturer is actually in the model column or the same things is written in different ways or with extra info at the end. There is not much consistency. If there was a direct instruction to use the 'model' or there was lots of time, then the 'model' column could be cleaned up to a degree. eg1. taking all letters to lower case. eg2: sorting by model and then if a reasonable length string before a space is in the cells below then just take the string up to the space on all that match it etc. Yet this would be very time consuming that did not return value for the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Leave out the 'model' column from 'cars' data frame\n",
    "cars = cars.drop(columns=[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Total reduction in row numbers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "How many rows have been removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count before: 213185\n",
      "Row count after: 213185\n",
      "Rows removed this step: 0\n",
      "Rows removed were actually zero since this is just a final total.\n",
      "Percentage removed in this step: 0.00%\n",
      "Percentage removed so far: 50.06%\n"
     ]
    }
   ],
   "source": [
    "len_before=len(cars)\n",
    "\n",
    "report_change(len_before,cars,\"actually zero since this is just a final total\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT OBSERVATIONS:** This 50% reduction in data seems like it is a lot but with messy data scraped from the internet reflecting real world data entered by humans, it is not uncommon. It is particularly important to note that at Step 12 above of data preparation, we removed 28% from the original full data set by the necessary step of simply removing multiple duplicates of the very same car - reducing down the cars that were listed in a high number of locations (their entry had the same VIN and everything else except location)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up pipline for numerical featrues and also for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Define which are the numerical columns and which are categorical:\n",
    "cat_features = ['manufacturer','condition', 'fuel',\n",
    "                'title_status','transmission','drive',\n",
    "                'size','type','paint_color','state'\n",
    "               ]\n",
    "num_features = ['year','odometer','cylinders']\n",
    "\n",
    "\n",
    "# Deal with NaN for the numeric by inserting the median\n",
    "numeric_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    PolynomialFeatures(include_bias=False),\n",
    "    # So we can use \n",
    "    StandardScaler() # Scale values\n",
    ")\n",
    "\n",
    "# Deal with NaN for categorial columns by inserting the most frequent \n",
    "categorical_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\")\n",
    ")\n",
    "\n",
    "# Preprocessor to implement the above \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipeline, num_features),\n",
    "        (\"cat\", categorical_pipeline, cat_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set price as the target and split the data into training and test sets (70% training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the target data as 'price' and the other columns to use:\n",
    "X = cars.drop(['price'], axis=1)\n",
    "y = cars['price']\n",
    "\n",
    "# Split into training and testing data (70% split):\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.3, random_state=22)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Set up mutiple model and hyperparameters - the GridSearchCV inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base pipeline: preprocessing + placeholder model (will be swapped by GridSearchCV)\n",
    "base_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", LinearRegression())\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "#              SUCCESSIVE PARAMETER GRID REFINEMENTS:\n",
    "#        Outcome for Refinement 1, informed Refinement 2, etc\n",
    "# =======================================================================\n",
    "\n",
    "param_grids = {\n",
    "\n",
    "    # ============================================================\n",
    "    # Refinement 1: Linear only + modest polynomial\n",
    "    # ============================================================\n",
    "    1: [\n",
    "        {\n",
    "            \"preprocessor__num__polynomialfeatures__degree\": [1],\n",
    "            \"model\": [LinearRegression()],\n",
    "        },\n",
    "\n",
    "        # 2) Polynomial regression (LinearRegression with higher degree)\n",
    "        {\n",
    "            \"preprocessor__num__polynomialfeatures__degree\": [2, 3],\n",
    "            \"model\": [LinearRegression()],\n",
    "        },\n",
    "\n",
    "        # 3) Ridge regression\n",
    "        {\n",
    "            \"preprocessor__num__polynomialfeatures__degree\": [1, 2, 3],\n",
    "            \"model\": [Ridge()],\n",
    "            \"model__alpha\": [0.1, 1.0, 10.0],\n",
    "        },\n",
    "\n",
    "        # 4) Lasso regression\n",
    "        {\n",
    "            \"preprocessor__num__polynomialfeatures__degree\": [1, 2, 3],\n",
    "            \"model\": [Lasso(max_iter=10000)],\n",
    "            \"model__alpha\": [0.0001, 0.001, 0.01],\n",
    "        },\n",
    "    ],\n",
    "\n",
    "    # ============================================================\n",
    "    # Refinement 2: Shift the polynomial search higher than 3\n",
    "    # ============================================================\n",
    "    2: [\n",
    "        {\n",
    "            \"preprocessor__num__polynomialfeatures__degree\": [1],\n",
    "            \"model\": [LinearRegression()],\n",
    "        },\n",
    "\n",
    "        # 2) Polynomial regression (LinearRegression with higher degree)\n",
    "        {\n",
    "            \"preprocessor__num__polynomialfeatures__degree\": [3,4,5,6,7],\n",
    "            \"model\": [LinearRegression()],\n",
    "        },\n",
    "\n",
    "        # 3) Ridge regression\n",
    "        {\n",
    "            \"preprocessor__num__polynomialfeatures__degree\": [3,4,5,6,7],\n",
    "            \"model\": [Ridge()],\n",
    "            \"model__alpha\": [0.1, 1.0, 10.0],\n",
    "        },\n",
    "\n",
    "        # 4) Lasso regression\n",
    "        {\n",
    "            \"preprocessor__num__polynomialfeatures__degree\": [3,4,5,6,7],\n",
    "            \"model\": [Lasso(max_iter=10000)],\n",
    "            \"model__alpha\": [0.0001, 0.001, 0.01],\n",
    "        },\n",
    "    ],\n",
    "\n",
    "    # ============================================================\n",
    "    # Refinement 3: Polynomial 5 + focussed Ridge regularisation\n",
    "    # ============================================================\n",
    "    3: [\n",
    "        {\n",
    "            \"preprocessor__num__polynomialfeatures__degree\": [1],\n",
    "            \"model\": [LinearRegression()],\n",
    "        },\n",
    "\n",
    "        # 2) Polynomial regression (LinearRegression with higher degree)\n",
    "        {\n",
    "            \"preprocessor__num__polynomialfeatures__degree\": [5],\n",
    "            \"model\": [LinearRegression()],\n",
    "        },\n",
    "\n",
    "        # 3) Ridge regression\n",
    "        {\n",
    "            \"preprocessor__num__polynomialfeatures__degree\": [5],\n",
    "            \"model\": [Ridge()],\n",
    "            \"model__alpha\": [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4, 1e-5],\n",
    "        },\n",
    "\n",
    "    ],\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Fit the grid to the training data for all the different iterations of parameters in param_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===== PARAMETER REFINEMENT SET NUMBER: 1 =====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is a loop of the 3 successive sets of hyperparameters\n",
    "# that were used to find the best model\n",
    "\n",
    "\n",
    "results = []  # store results from each parameter grid refinement number\n",
    "\n",
    "for parameter_refinement_number in [1, 2, 3]:\n",
    "\n",
    "    print()\n",
    "    print(f\"\\n===== PARAMETER REFINEMENT SET NUMBER: {parameter_refinement_number} =====\")\n",
    "    print()\n",
    "\n",
    "    # Assign the parameter grid according to the 'parameter grid' refinement number:\n",
    "    param_grid = param_grids[parameter_refinement_number]\n",
    "\n",
    "    # Grid search with 5-fold CV, optimising RMSE (negative because sklearn maximises the score)\n",
    "    grid = GridSearchCV(\n",
    "        estimator=base_pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # START a timer\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # FIT = RUN all the different iterations (models) in the chosen 'parameter grid'\n",
    "    grid.fit(X_train, y_train)\n",
    "      \n",
    "    # END a timer\n",
    "    end = time.perf_counter()\n",
    "    elapsed = end - start\n",
    "    hours = int(elapsed // 3600)\n",
    "    minutes = int((elapsed % 3600) // 60)\n",
    "    seconds = int(elapsed % 60)\n",
    "\n",
    "    # ---- RESULTS: ----\n",
    "    best_cv_rmse = -grid.best_score_\n",
    "    best_params = grid.best_params_\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    # ---- PRINT THIS PARAMETER SET RESULTS: ----\n",
    "    print(f\"GridSearchCV runtime: {hours}h {minutes}m {seconds}s\")\n",
    "    print(\"Best params:\", best_params)\n",
    "    print(\"Best RMSE from CV (Cross-Validation) on TRAINING data set:\", best_cv_rmse)\n",
    "    print(f\"Runtime: {elapsed:.1f} seconds\")\n",
    "\n",
    "    # ---- SAVE RESULTS IN A LIST WITH EACH PARAMETER GRID RESULT: ----\n",
    "    results.append({\n",
    "        \"parameter_refinement_number\": parameter_refinement_number,\n",
    "        \"best_cv_rmse\": best_cv_rmse,\n",
    "        \"best_params\": best_params,\n",
    "        \"runtime_seconds\": elapsed,\n",
    "        \"cv_results\": grid.cv_results_, \n",
    "        \"best_estimator\": grid.best_estimator_,   # <-- add this\n",
    "    })\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Model comparison by looking at RMSE for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for run in results:\n",
    "    refinement = run[\"parameter_refinement_number\"]\n",
    "    cv_results = run[\"cv_results\"]\n",
    "\n",
    "    for params, mean_score in zip(cv_results[\"params\"], cv_results[\"mean_test_score\"]):\n",
    "        rows.append({\n",
    "            \"parameter_refinement_number\": refinement,\n",
    "            \"Model\": type(params[\"model\"]).__name__,\n",
    "            \"Degree\": params[\"preprocessor__num__polynomialfeatures__degree\"],\n",
    "            \"Alpha\": params.get(\"model__alpha\", np.nan),\n",
    "            \"CV_RMSE\": -mean_score\n",
    "        })\n",
    "\n",
    "df_compare = (\n",
    "    pd.DataFrame(rows)\n",
    "      .sort_values([\"parameter_refinement_number\", \"CV_RMSE\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "#============================================================================================\n",
    "#  Work out and print the overall top_n_max performances for all parameter refinement sets:\n",
    "#============================================================================================\n",
    "# A. How many of the top performances do we want to see?\n",
    "top_n_max = 10\n",
    "\n",
    "# B. Work out the overall top performing models:\n",
    "df_top_unique = (\n",
    "    df_compare\n",
    "    .sort_values(\"CV_RMSE\", ascending=True)\n",
    "    .drop_duplicates(subset=[\"Model\", \"Degree\", \"Alpha\"], keep=\"first\")\n",
    "    .head(top_n_max)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# C. Print out the overal top performing models:\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"The ranked top {top_n_max} performing models across all parameter sets (lowest CV RMSE):\")\n",
    "print(\"=\" * 80)\n",
    "df_top_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best performing model\n",
    "\n",
    "The top row of the table above has the lowest cross-validation room mean squared error (CV_RMSE). The linear regression model of polynomial degree 5 provided the most improvement in the CV_RMSE (compared with polynomials of other degrees). Applying the ridge regression and fine tuning the alpha value only slightly further improved the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "With some modeling accomplished, we aim to reflect on what we identify as a high-quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight into drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How does the selected best model perform on the test data? - focus for data analysts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect the details of the run with the lowest TRAINING SET CV RMSE\n",
    "best_run = min(results, key=lambda d: d[\"best_cv_rmse\"])\n",
    "best_model = best_run[\"best_estimator\"]\n",
    "\n",
    "# Now apply the best model once to the TEST set for the FIRST TIME:\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print()\n",
    "print(\"=\" * 64)\n",
    "print(\"  THE BEST PERFORMING MODEL, APPLIED TO THE HELD-OUT TEST SET:\")\n",
    "print(\"=\" * 64)\n",
    "print(\"Final selected model (by training data CV):\")\n",
    "print(\"Parameter set:\", best_run[\"parameter_refinement_number\"])\n",
    "print(\"Best params:\", best_run[\"best_params\"])\n",
    "print(f\"TRAINING CV RMSE:   {best_run[\"best_cv_rmse\"]:.1f}\")\n",
    "print(f\"Held-out TEST RMSE: {test_rmse:.1f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OBSERVATIONS: The selected best model's performance:\n",
    "The held-out TEST RMSE is better than the TRAINING RMSE so that is encourgaing that the model did not overfit on the training data and has a degree of versatility for new data. If the training RMSE had been significantly worse than the training data then this would be a clear sign of overfitting to the particularities of the test data and the selected model not being a generalised solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Which polynomial features most influence the model? - focus for data analysts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Extract preprocessor and feature names ---\n",
    "pre = best_model.named_steps[\"preprocessor\"]\n",
    "\n",
    "num_features_trans = (\n",
    "    pre.named_transformers_[\"num\"]\n",
    "       .named_steps[\"polynomialfeatures\"]\n",
    "       .get_feature_names_out(input_features=num_features)\n",
    ")\n",
    "\n",
    "cat_features_trans = (\n",
    "    pre.named_transformers_[\"cat\"]\n",
    "       .named_steps[\"onehotencoder\"]\n",
    "       .get_feature_names_out(input_features=cat_features)\n",
    ")\n",
    "\n",
    "feature_names = np.concatenate([num_features_trans, cat_features_trans])\n",
    "\n",
    "# --- Coefficients (works for LinearRegression, Ridge, Lasso) ---\n",
    "coef = best_model.named_steps[\"model\"].coef_\n",
    "\n",
    "importance_df = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"coefficient\": coef,\n",
    "        \"abs_importance\": np.abs(coef)\n",
    "    })\n",
    "    .sort_values(\"abs_importance\", ascending=False)\n",
    ")\n",
    "\n",
    "print()\n",
    "print(f\" Best model, number of polynomial features created:\", len(feature_names))\n",
    "print(\"=\" * 57)\n",
    "print(\"  THE MOST IMPORTANT POLYNOMIAL FEATURES FOR THE MODEL:\")\n",
    "print(\"=\" * 57)\n",
    "\n",
    "importance_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The most influential polynomial features:\n",
    "1. These top ranking polynomial features are mostly to the power of 2 and 3 with only one in the top 10 being to the power of 5 which was the polynomial degree of the overall best solution. It is however ranked 4th which shows it as fairly significiant. It is the year of manufacture raised to the power of 5. We might expect the year to have an important place in our model - since old vintage cars are often valuable, new cars are also valuable and there is an in between period where the relationship is not straightforward - so we can start to see that year is a complex and non-linear factor. Also, a single quadratic curve will also not effectively fit the complexity of the year.\n",
    "2. Odometer also features in the top polynomial features, along with cylinders. We might also have expected odometer to rank highly since it is intuitive that cars with low miles/kilometers have less wear and tear.\n",
    "3. Cylinders is a non-linear feature since a 5 cylinder vehicle is a rare car so it's unlikely that its price is expected to lie between 4 and 6 cylinder cars, which are more common. We can see cylinders featuring in numerous polynomial features to the power of 2 and 3 indicating a non-linear result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Which original main features most influence the model? - client focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ======================================================================\n",
    "#     Map the expanded feature names back to original base features\n",
    "# ======================================================================\n",
    "\n",
    "def map_back_to_original(feature_name):\n",
    "    # numeric: \"year\", \"year^2\", \"year odometer\"\n",
    "    for col in num_features:\n",
    "        if col in feature_name:\n",
    "            return col\n",
    "    \n",
    "    # categorical: \"type_SUV\", \"fuel_diesel\"\n",
    "    for col in cat_features:\n",
    "        if feature_name.startswith(col):\n",
    "            return col\n",
    "    \n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "# ------------- Data frame with the original features: -------------\n",
    "importance_df[\"original_feature\"] = \\\n",
    "                    importance_df[\"feature\"].apply(map_back_to_original)\n",
    "\n",
    "# --------------------- Build importance table ---------------------\n",
    "importance_table = (\n",
    "    importance_df.groupby(\"original_feature\")[\"abs_importance\"]\n",
    "                 .sum()\n",
    "                 .sort_values(ascending=False)\n",
    "                 .reset_index()\n",
    ")\n",
    "# ------------------------- Round + format -------------------------\n",
    "importance_table[\"Importance (Rounded)\"] = \\\n",
    "                importance_table[\"abs_importance\"].round(0).astype(int)\n",
    "importance_table[\"Importance (Formatted)\"] = \\\n",
    "                importance_table[\"Importance (Rounded)\"].apply(lambda x: f\"{x:,}\")\n",
    "\n",
    "# ---------- Keep only Feature, Raw Importance, Formatted ----------\n",
    "importance_table = importance_table[[\n",
    "    \"original_feature\",\n",
    "    \"abs_importance\",\n",
    "    \"Importance (Formatted)\"\n",
    "]]\n",
    "\n",
    "importance_table = importance_table.rename(columns={\n",
    "    \"original_feature\": \"Feature\",\n",
    "    \"abs_importance\": \"Raw Importance\"\n",
    "})\n",
    "\n",
    "# ----------------------- Display the table ------------------------\n",
    "\n",
    "winning_degree = best_run[\"best_params\"][\"preprocessor__num__polynomialfeatures__degree\"]\n",
    "\n",
    "print(\"=\" * 62)\n",
    "print(f\"  MOST IMPORTANT CAR FEATURES: model's polynomial degree = {winning_degree}\")\n",
    "print(\"=\" * 62)\n",
    "display(importance_table.head(7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "The importance of year is far greater than the next feature of cylinders. Cylinders and odometer are not too far apart, especially in terms of order of magnitude. Then manufacturer is the 4th most influential feature but significantly lower than odometer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Graphically, how much do the original main features influence the model? - client focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------- Horizontal bar chart ----------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.barh(\n",
    "    importance_table[\"Feature\"],\n",
    "    importance_table[\"Raw Importance\"],\n",
    "    color=\"skyblue\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Raw Importance (sum of absolute coefficients)\")\n",
    "plt.title(f\"Relative Feature Importance (Original Features), for polynomial degree = {winning_degree}\")\n",
    "plt.gca().invert_yaxis()   # Highest importance at top\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Observations:\n",
    "\n",
    "Graphically, we confirm that the importance of 'year' is far greater than the next feature of 'cylinders'. 'Cylinders' and 'odometer' are not too far apart (same order of magnitude). Then manufacturer is the 4th most influential feature but significantly lower than odometer. Compared with 'year', the 'manufacturer' is too small to event register on the bar chart above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplified and cleaned chart:\n",
    "\n",
    "Cleaned up chart to use in the final report (under 'Deployment'):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#----------------- Clean and simplify chart for clients -----------------\n",
    "top_n = 7\n",
    "\n",
    "plot_df = importance_table.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.barh(plot_df[\"Feature\"], plot_df[\"Raw Importance\"])\n",
    "\n",
    "plt.xlabel(\"Raw Importance (mathematically calculated from the car data)\")\n",
    "plt.title(\"Relative Importance of car features\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "#----- Save chart in a new image directory for clients' final report -----\n",
    "\n",
    "outfile = \"relative_importance_top7.jpg\"\n",
    "outdir = \"images_of_results\"\n",
    "\n",
    "os.makedirs(\"images_of_results\", exist_ok=True)\n",
    "outfile = os.path.join(outdir, \"relative_importance_top7.jpg\")\n",
    "plt.savefig(outfile, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Print out where the file is saved (i.e. the current working directory path:\n",
    "# import os\n",
    "# print(\"Saved to:\", os.path.abspath(\"relative_importance_top7.jpg\"))\n",
    "\n",
    "#----------------- Display the simplified chart here also -----------------\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. The 'year' of manufacture's pattern in relation to price- client and data anylist focus\n",
    "###    (What is the shape for the most influential feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ---- 1. Scatter plot of raw data ----\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(cars[\"year\"], cars[\"price\"], alpha=0.1, s=5, label=\"Actual Data\")\n",
    "\n",
    "# ---- 2. Build a synthetic dataset for prediction curve ----\n",
    "\n",
    "# Create a smooth range of years\n",
    "year_range = np.linspace(cars[\"year\"].min(), cars[\"year\"].max(), 200)\n",
    "\n",
    "# Build a DataFrame that matches my model's input structure\n",
    "curve_df = pd.DataFrame({\n",
    "    \"year\": year_range,\n",
    "    \"odometer\": cars[\"odometer\"].median(),      # fix odometer at typical value\n",
    "    \"cylinders\": cars[\"cylinders\"].median(),    # fix cylinders at typical value\n",
    "    \"manufacturer\": \"ford\",                     # pick ANY valid category\n",
    "    \"condition\": \"good\",\n",
    "    \"fuel\": \"gas\",\n",
    "    \"title_status\": \"clean\",\n",
    "    \"transmission\": \"automatic\",\n",
    "    \"drive\": \"fwd\",\n",
    "    \"size\": \"mid-size\",\n",
    "    \"type\": \"sedan\",\n",
    "    \"paint_color\": \"white\",\n",
    "    \"state\": \"ca\"\n",
    "})\n",
    "\n",
    "# ---- 3. Predict prices using my best model ----\n",
    "predicted_prices = best_model.predict(curve_df)\n",
    "\n",
    "# ---- 4. Plot the prediction curve ----\n",
    "plt.plot(year_range, predicted_prices, color=\"red\", linewidth=3, label=\"Model Prediction\")\n",
    "\n",
    "# ---- Labels ----\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Price ($)\")\n",
    "plt.title(\"Year vs Price with Model Prediction Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "When we examined the most influential polynominal features, we saw these near the top of the list:\n",
    "\n",
    "- year<sup>3</sup> x cylinders<sup>2</sup>\n",
    "- year x cylinders<sup>2</sup>\n",
    "- year<sup>5</sup>\n",
    "\n",
    "\n",
    "As we look at the curve of the year and its affect on price, we can see the complexity of the curve that that needed a polynomial of degree 5 to raise year to the power of 5. A simple linear solution would not fit well, nor would a simple quadratic (year<sup>2</sup>) since this would only allow for one downward curve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Have we met the business goal of this investigation? Not yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "It is time to review whether we are closer to meeting the business goal.\n",
    "\n",
    "**Business Goal:** to understand what factors make a car more or less expensive - what do consumers value in a used car?\n",
    "\n",
    "So we can now say that these features most affect the price:\n",
    "- Year, cylinders, odometer and manufacturer\n",
    "\n",
    "We now also know more specifically for the highly influential feature of 'year', the value starts to drop away for cars around 20 years old (see graph above) but value is added to the price for cars before 1980 (or 1970).\n",
    "\n",
    "This is helpful. However, we fall short currently for knowing what number of cylinders is most valued, as with which kinds of odometer readings customer's value and especially which brands of 'manufacturer' the customer's value the most.\n",
    "\n",
    "We will now seek to determine these things.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Which VALUES of the original main features are most valued? - client focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "top_features = 5\n",
    "value_number = 3\n",
    "table_output_file_name = \"top_features_table.jpg\"\n",
    "\n",
    "top_feature_list = importance_table[\"Feature\"].head(top_features).tolist()\n",
    "values_tables = []\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#     Function: format a number for dollars and cents \n",
    "# ------------------------------------------------------------\n",
    "def format_currency(x):\n",
    "    return \"$\" + format(x, \",.2f\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "#     Function: compute ranking for one feature (cat or num)\n",
    "# ----------------------------------------------------------------\n",
    "def feature_ranking(cars, col):\n",
    "    if col in cat_features:\n",
    "        ranking = cars.groupby(col)[\"price\"].mean().sort_values(ascending=False)\n",
    "        \n",
    "\n",
    "    elif col in num_features:\n",
    "        if col == \"odometer\":\n",
    "            # bin odometer into deciles, then rank bins by average price\n",
    "            binned = pd.qcut(cars[col], q=10, duplicates=\"drop\")\n",
    "            ranking = cars.groupby(binned,observed=True)[\"price\"].mean().sort_values(ascending=False)\n",
    "    \n",
    "        elif col == \"year\":\n",
    "            # bin year into 5-year buckets (e.g., 20002004, 20052009, ...)\n",
    "            year_min = cars[col].min()\n",
    "            year_max = cars[col].max()\n",
    "\n",
    "            # create bin edges aligned to 5-year boundaries\n",
    "            start = (year_min // 5) * 5\n",
    "            end = ((year_max // 5) + 1) * 5\n",
    "\n",
    "            bins = list(range(int(start), int(end) + 1, 5))\n",
    "            labels = [f\"{bins[i]}{bins[i+1]-1}\" for i in range(len(bins)-1)]\n",
    "\n",
    "            binned = pd.cut(cars[col], bins=bins, right=False, labels=labels, include_lowest=True)\n",
    "            ranking = cars.groupby(binned, observed=True)[\"price\"].mean().sort_values(ascending=False)\n",
    "\n",
    "        else:\n",
    "            ranking = cars.groupby(col)[\"price\"].mean().sort_values(ascending=False)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"{col} not found in cat_features or num_features\")\n",
    "    \n",
    "    # Keep only the top value_number entries\n",
    "    ranking = ranking.head(value_number)\n",
    "\n",
    "    # Format output\n",
    "    ranking_df = ranking.to_frame(name=\"Average Price\")\n",
    "    ranking_df[\"Average Price\"] = ranking_df[\"Average Price\"].apply(format_currency)\n",
    "\n",
    "    return ranking_df\n",
    "\n",
    "    \n",
    "# ----------------------------------------------------------------\n",
    "#       Print rankings for only the selected top features\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "for col in top_feature_list:\n",
    "    ranking_df = feature_ranking(cars, col)\n",
    "    if ranking_df is None:\n",
    "        continue\n",
    "\n",
    "    # IMPORTANT: keep index as a column\n",
    "    df = ranking_df.reset_index()\n",
    "\n",
    "    # Rename the index column to the feature name (e.g., year, cylinders)\n",
    "    df.columns = [col, \"Average Price\"]\n",
    "\n",
    "    values_tables.append((col.capitalize(), df))\n",
    "\n",
    "\n",
    "# ---------- Make figure big enough ----------\n",
    "total_rows = sum(len(df) for _, df in values_tables)\n",
    "fig_height = 1.5 + 0.55 * total_rows + 0.9 * len(values_tables)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, fig_height))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "y = 0.98  # start near top\n",
    "\n",
    "for title, df in values_tables:\n",
    "    # section title\n",
    "    ax.text(0.01, y, f\"{title} (top {value_number} values)\", fontsize=12, fontweight=\"bold\",\n",
    "            transform=ax.transAxes, va=\"top\")\n",
    "    y -= 0.04\n",
    "\n",
    "    # draw table\n",
    "    row_h = 0.028\n",
    "    h = row_h * (len(df) + 1)\n",
    "\n",
    "    tbl = ax.table(\n",
    "        cellText=df.values,\n",
    "        colLabels=df.columns,\n",
    "        cellLoc=\"left\",\n",
    "        colLoc=\"left\",\n",
    "        loc=\"upper left\",\n",
    "        bbox=[0.01, y - h, 0.98, h]\n",
    "    )\n",
    "\n",
    "    tbl.auto_set_font_size(False)\n",
    "    tbl.set_fontsize(10)\n",
    "    tbl.scale(1, 1.1)\n",
    "\n",
    "    # Bold column headers\n",
    "    for (r, c), cell in tbl.get_celld().items():\n",
    "        if r == 0:\n",
    "            cell.set_text_props(weight=\"bold\")\n",
    "\n",
    "    y -= (h + 0.06)\n",
    "\n",
    "\n",
    "# ------ Save the table of the top features and their values ------\n",
    "\n",
    "outdir = \"images_of_results\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "outfile = os.path.join(outdir, table_output_file_name)\n",
    "\n",
    "plt.savefig(outfile, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# print(f\"Saved summary image to: {outfile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images_of_results/top_features_table.jpg\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion:\n",
    "\n",
    "Our business goal is:\n",
    "\n",
    "**Business Goal:** to understand what factors make a car more or less expensive - what do consumers value in a used car?\n",
    "\n",
    "Our conclusion is that four features most significantly affect the price. Other factors affect the price but to significantly less degree. We now also know what in particular about those features are valued the most. In order of most influential:\n",
    "1. **year**\n",
    "   The most valued 10 year period is 2020-2029, followed by 1910-1919, then 1930-1939.\n",
    "   \n",
    "3. **cylinders**\n",
    "   The most valued are 12 cylinder vehicles, followed by 8 cylinder vehicles.\n",
    "\n",
    "   \n",
    "5. **odometer**\n",
    "   The most valued are cars with under 24,471 km on the odometer (top decile = top 10%).\n",
    "\n",
    "   \n",
    "7. **manufacturer**\n",
    "    The most valued manufactuers are: Ferrari, Aston-Marton and Tesla.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine-tuning their inventory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Report for Client:\n",
    "\n",
    "\n",
    "\n",
    "**Business Goal:** to understand what factors make a car more or less expensive - what do consumers value in a used car?\n",
    "\n",
    "#### 1. Car features that weigh most heavily on car price:\n",
    "\n",
    "Our conclusion is that four features most significantly affect the price. Other factors affect the price but to significantly less degree.\n",
    "\n",
    "We have confirmed that the importance of 'year' is far greater than the next feature of 'cylinders'. 'Cylinders' and 'odometer' are not too far apart (same order of magnitude). Then manufacturer is the fourth most influential feature but significantly lower than odometer. Compared with 'year', the 'manufacturer' is too small to even register on the bar chart below.\n",
    "\n",
    "<img src=\"images_of_results/relative_importance_top7.jpg\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Values that affect the price for the most influential car features:\n",
    "\n",
    "We now also know what entries in particular for those features are valued the most. Starting with the most influential and then listing others with decreasing influence, they are:\n",
    "1. **year**\n",
    "   The most valued 10 year period is 2020-2029, followed by 1910-1919, then 1930-1939.\n",
    "   \n",
    "3. **cylinders**\n",
    "   The most valued are 12 cylinder vehicles, followed by 8 cylinder vehicles.\n",
    "\n",
    "   \n",
    "5. **odometer**\n",
    "   The most valued are cars with under 24,471 km on the odometer (top decile = top 10%).\n",
    "\n",
    "   \n",
    "7. **manufacturer**\n",
    "    The most valued manufactuers are: Ferrari, Aston-Marton and Tesla.\n",
    "\n",
    "\n",
    "<img src=\"images_of_results/top_features_table.jpg\" width=\"550\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda base)",
   "language": "python",
   "name": "conda-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
